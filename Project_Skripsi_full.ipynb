{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_Skripsi_full.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbVdSRPoyhrFBE6qyeX870",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndhartanto/ndhartanto/blob/main/Project_Skripsi_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qH3fkNpRyaV",
        "outputId": "c1de5296-157d-4e3d-968a-7b9a35b2b476"
      },
      "source": [
        "!pip3 install -qq --upgrade git+https://github.com/twintproject/twint.git@origin/master#egg=twint\n",
        "!pip install -qq nest_asyncio\n",
        "!pip install goslate \n",
        "\n",
        "import twint\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import csv\n",
        "import re\n",
        "import goslate\n",
        "\n",
        "gs = goslate.Goslate()\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# creating object of SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "import spacy\n",
        "import string\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# for stopwords removal\n",
        "from spacy.lang.en import English\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = English()\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 5.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 266kB 18.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 21.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 23.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 25.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 296kB 24.0MB/s \n",
            "\u001b[?25h  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BWh2k5ESCjW"
      },
      "source": [
        "# HEADING FOR THE CSV \n",
        "\n",
        "csvFile = open('data_perdagangan.csv', 'w')\n",
        "\n",
        "csvWriter = csv.writer(csvFile)\n",
        "csvWriter.writerow([\"Date & Time\", \"Username\", \"Tweet\"])\n",
        "csvFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQmWH-GPSIcZ"
      },
      "source": [
        "# CONTENT \n",
        "\n",
        "keywords = [\"perdagangan indonesia china\",\n",
        "            \"perdagangan indonesia cina\",]\n",
        "\n",
        "for x in keywords :\n",
        "\n",
        "  c = twint.Config()\n",
        "  c.Search = x \n",
        "  c.Limit = 2000\n",
        "  c.Since = \"2020-03-30\"\n",
        "  c.Until = \"2021-03-30\"\n",
        "  c.Store_object = True\n",
        "  c.Hide_output = True\n",
        "  c.Filter_retweets = True\n",
        "  c.Lang = \"ID\"\n",
        "\n",
        "  twint.run.Search(c)\n",
        "  tweets = twint.output.tweets_list\n",
        "\n",
        "def remove_duplicates(tweets):\n",
        "  newTweets = []\n",
        "  ids = []\n",
        "\n",
        "  for tweet in tweets:\n",
        "    if (tweet.id not in ids):\n",
        "      ids.append(tweet.id)\n",
        "      newTweets.append(tweet)\n",
        "    return newTweets\n",
        "\n",
        "tweets = remove_duplicates(tweets)\n",
        "\n",
        "csvFile = open('data_perdagangan.csv', 'a')\n",
        "\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "for tweet in tweets:\n",
        "  csvWriter.writerow([tweet.datetime, tweet.username, tweet.tweet])\n",
        "  print(tweet.datetime, tweet.username, tweet.tweet)\n",
        "csvFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh8KppCaME0G"
      },
      "source": [
        "# CLEANING \n",
        "\n",
        "csvFile = open('data_perdagangan.csv', 'r', encoding=\"utf8\", errors='ignore')\n",
        "newFile = open('data_clean.csv', 'w')\n",
        "rows = csv.reader(csvFile)\n",
        "csvWriter = csv.writer(newFile)\n",
        "\n",
        "def remove_url(txt):\n",
        "    \"\"\"Replace URLs found in a text string with nothing \n",
        "    (i.e. it will remove the URL from the string).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    txt : string\n",
        "        A text string that you want to parse and remove urls.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    The same txt string with url's removed.\n",
        "     SOURCE : https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/calculate-tweet-word-frequencies-in-python/\n",
        "    \"\"\"\n",
        "\n",
        "    return \" \".join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", txt).split())\n",
        "\n",
        "def remove_usernames(txt):\n",
        "    \"\"\" removing twitter usernames / mentions \n",
        "    (example : \"Hello @user123 how are you\" becomes \"Hello how are you\")\n",
        "    SOURCE : https://stackoverflow.com/questions/50830214/remove-usernames-from-twitter-data-using-python\"\"\"\n",
        "\n",
        "    return re.sub('@[\\w]+','',txt)\n",
        "\n",
        "def remove_hashtags(txt):\n",
        "    \"\"\"removing hashtags\n",
        "    (example : \"Hello #howareyou\" becomes \"Hello\")\"\"\"\n",
        "    return re.sub('#[\\w]+','',txt)\n",
        "\n",
        "def remove_numbers(txt):\n",
        "    \"\"\"removing numbers\n",
        "    (example : \"Hello 12 how are you\" becomes \"Hello how are you\")\"\"\"\n",
        "    return re.sub(r'[0-9\\.]+', '', txt)\n",
        "\n",
        "def remove_entity(txt):\n",
        "    \"\"\"removing special HTML entity such as &gt; , &amp; , etc\"\"\"\n",
        "    return re.sub('&[\\w]+;', '', txt)\n",
        "\n",
        "for row in rows:\n",
        "  tweet = row[2] \n",
        "  tweet = remove_usernames(tweet)\n",
        "  tweet = remove_hashtags(tweet)\n",
        "  tweet = remove_entity(tweet)\n",
        "  tweet = remove_url(tweet)\n",
        "  tweet = remove_numbers(tweet)\n",
        "\n",
        "  print(tweet)\n",
        "  row.append(tweet)\n",
        "  csvWriter.writerow(row)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRtszzOYMIr7"
      },
      "source": [
        "# TRANSLATING\n",
        "\n",
        "csvFile = open('data_clean.csv', 'r') #, encoding=\"utf8\", errors='ignore')\n",
        "newFile = open('data_trans.csv', 'w')\n",
        "rows = csv.reader(csvFile)\n",
        "csvWriter = csv.writer(newFile)\n",
        "\n",
        "for row in rows:\n",
        "  sentence = row[3] \n",
        "  sentence = gs.translate(sentence, 'en')\n",
        "  sentence.lower()\n",
        "  print(sentence)\n",
        "  row.append(sentence)\n",
        "  csvWriter.writerow(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H73yIkCZMm2S"
      },
      "source": [
        "# VADER POLARITY \n",
        "\n",
        "# loading csv file.\n",
        "df = pd.read_csv('tweet_eng_only.csv', encoding='latin-1')\n",
        "\n",
        "df.head()\n",
        "\n",
        "# Checking\n",
        "# sid.polarity_scores(df.iloc[0]['text'])\n",
        "\n",
        "df['scores'] = df['text'].apply(lambda review:sid.polarity_scores(review))\n",
        "df['compound'] = df['scores'].apply(lambda d:d['compound'])\n",
        "df['label'] = df['compound'].apply(lambda score: 'POSITIF' if score >0 else 'NEGATIF' if score <0 else 'NETRAL')\n",
        "df.head()\n",
        "\n",
        "# df.to_csv('labeled_vader.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC6dtPlWNI0Y"
      },
      "source": [
        "# LEMMATIZATION & STOPWORDS REMOVAL\n",
        "\n",
        "csvFile = open('labeled_vader.csv', 'r')\n",
        "newFile = open('spacy_vader_LEMMA.csv', 'w')\n",
        "rows = csv.reader(csvFile)\n",
        "csvWriter = csv.writer(newFile)\n",
        "\n",
        "for row in rows:\n",
        "  # lower case\n",
        "  sentence = row[0].lower()\n",
        "\n",
        "  # remove punctuation if any\n",
        "  sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "  sentence = sp(sentence)\n",
        "    \n",
        "  # LEMMATISATION\n",
        "  lemma_list = []\n",
        "  for word in sentence:\n",
        "     # CHECK\n",
        "     # print(word.text, word.lemma_)\n",
        "     lemma_list.append(word.lemma_)\n",
        "\n",
        "  no_stopwords =[] \n",
        "\n",
        "  for word in lemma_list:\n",
        "    lexeme = nlp.vocab[word]\n",
        "    if lexeme.is_stop == False:\n",
        "      no_stopwords.append(word) \n",
        "  print(lemma_list)\n",
        "  print(no_stopwords)   \n",
        "\n",
        "  row.append(lemma_list)\n",
        "  row.append(no_stopwords)\n",
        "  csvWriter.writerow(row)\n",
        "\n",
        "df = pd.read_csv('spacy_vader_LEMMA.csv') #,sep='\\t')\n",
        "df_new = df.rename(columns={\"['text'].1\" : 'text_final'})\n",
        "df_new.head() #CHECK\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd2qLF87NsJP"
      },
      "source": [
        "# CONFUSION MATRIX - Accuracy, Precision, Recall, Fscore\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "\n",
        "# HOLD-OUT - Dividing testing and training data\n",
        "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(df_new['text_final'],df_new['label'],test_size=0.3)\n",
        "\n",
        "# Label encode - transform Categorical data of string (negatif, netral, positif) into numerical values (0,1,2)\n",
        "Encoder = LabelEncoder()\n",
        "Train_Y = Encoder.fit_transform(Train_Y)\n",
        "Test_Y = Encoder.fit_transform(Test_Y)\n",
        "\n",
        "# TF-IDF Vectorizer \n",
        "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
        "Tfidf_vect.fit(df_new['text_final'])\n",
        "\n",
        "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
        "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n",
        "\n",
        "# CHECKING\n",
        "# print(Tfidf_vect.vocabulary_)\n",
        "# print(Train_X_Tfidf)\n",
        "\n",
        "# NAIVE BAYES & SVM\n",
        "\n",
        "# Classifier - Algorithm - Naive Bayes\n",
        "\n",
        "# fit the training dataset on the classifier\n",
        "Naive = naive_bayes.MultinomialNB()\n",
        "Naive.fit(Train_X_Tfidf,Train_Y)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
        "\n",
        "# Classifier - Algorithm - SVM\n",
        "\n",
        "# fit the training dataset on the classifier\n",
        "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
        "SVM.fit(Train_X_Tfidf,Train_Y)\n",
        "\n",
        "# predict the labels on validation dataset\n",
        "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
        "\n",
        "conf_matrix = confusion_matrix(Test_Y, predictions_SVM)\n",
        "class_label = [\"NEGATIF\", \"NETRAL\", \"POSITIF\"]\n",
        "test = pd.DataFrame(conf_matrix, index = class_label, columns = class_label)\n",
        "sns.heatmap(test, annot = True, fmt = \"d\")\n",
        "plt.title(\"Confusion Matrix SVM\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# SVM RESULT\n",
        "print(\"SVM RESULT\")\n",
        "print(\"Accuracy Score =\",accuracy_score(predictions_SVM, Test_Y))\n",
        "precision, recall, fscore, support = score(Test_Y, predictions_SVM, average='weighted')\n",
        "print(\"Precision =\", precision)\n",
        "print(\"Recall =\", recall)\n",
        "print(\"F-score =\", fscore)\n",
        "\n",
        "conf_matrix = confusion_matrix(Test_Y, predictions_NB)\n",
        "class_label = [\"NEGATIF\", \"NETRAL\", \"POSITIF\"]\n",
        "test = pd.DataFrame(conf_matrix, index = class_label, columns = class_label)\n",
        "sns.heatmap(test, annot = True, fmt = \"d\")\n",
        "plt.title(\"Confusion Matrix Naive Bayes\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# NAIVE BAYES RESULT\n",
        "print(\"NAIVE BAYES RESULT\")\n",
        "print(\"Accuracy Score =\",accuracy_score(predictions_NB, Test_Y))\n",
        "precision, recall, fscore, support = score(Test_Y, predictions_NB, average='weighted')\n",
        "print(\"Precision =\", precision)\n",
        "print(\"Recall =\", recall)\n",
        "print(\"F-score =\", fscore)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}